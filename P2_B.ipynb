{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just like in P2(a), perform POS Tagging on the Brown corpus. (Like before, train your Logistic Regression model on the\n",
    "#tagged corpus, and test on the untagged one). \n",
    "#Use one vs all logistic regression to perform this exercise. \n",
    "#Essentially, given a word, try to classify it with classifiers trained for all pos tags and get most probable one.\n",
    "#Do NOT use any ML libraries like scipy for coding up the logistic regression. NLTK maybe allowed, but only for \n",
    "#getting corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#one way is using class (optional), advantage being you can create multiple instances of class and train for each pos tag\n",
    "\n",
    "class logisitic_regression:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def train(data,pos_tag):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another crude way make train function for each class\n",
    "def train_for_class_A(data):\n",
    "    #train classifier for each pos tag in one vs all; in this particular case one will be class A and \n",
    "    #other class is rest\n",
    "    pass\n",
    "\n",
    "def train_for_class_B(data):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_corpus(corp):\n",
    "    #Read the Brown Corpus\n",
    "    #Take in one sentence at a time\n",
    "    tokenize_text(sentence)\n",
    "    pass\n",
    "\n",
    "#Consider clas as positive class, and the rest as negative, and perform LR for the given token tok\n",
    "def Logistic(tok, clas):\n",
    "    pass\n",
    "\n",
    "def Multi_Logistic():\n",
    "    read_corpus(corpus)\n",
    "    #code up one-many Logistic Regression (LR).\n",
    "    #Feed in the list of tokens to return the list of tags.\n",
    "    #Essentially take one of the classes as positive, and remaining as negative, and perform the standard LR.\n",
    "    #Repeat this for all the classes.\n",
    "    for token in tokens:\n",
    "        for class1 in classes:\n",
    "            Logistic(token, class1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 : Predict tag sequence and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_tag_sequence(sentence):\n",
    "    # Given a sentence, Get sequence of tags for it by getting most prefered tag for each word given  \n",
    "    # Feel free to add helper functions more features ( like say trigram w1w2w3 as features for w2)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
